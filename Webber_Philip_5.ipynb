{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/philthebrave/Secret-Gists/blob/master/Webber_Philip_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8_8xLvOK4AS"
      },
      "source": [
        "# This provides a framework for functions that will be tested as part of module 5 NLP\n",
        "# You should test your submissions against the cases listed below, they will then be\n",
        "# tested against further unseen cases before being reviewed manually."
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luyWe9n6K_u5"
      },
      "source": [
        "**THE ASSIGNMENT**\n",
        "\n",
        "Populate the notebook below to create functions that achieve the following tasks.\n",
        "They must also pass the tests included at the bottom of the notebook.\n",
        "**Part 1**: produce a scraper function that can return the following information when given a URL from the BBC news page.  This function must be iterable - it can be used in a loop to examine a number of URLs and return the following information as a JSON.\n",
        "\n",
        "a) URL (provided.  For example https://www.bbc.co.uk/news/uk-51004218)\n",
        "\n",
        "b) Title\n",
        "\n",
        "c) Date\n",
        "\n",
        "d) Content (the main body of article)\n",
        "\n",
        "**Part 2**:  Write a function that when given a block of text (as a string)  returns all the following entities in a json object,  It is suggested that you use a pre-built or custom entity recogniser rather than a rules based method.  There are entity recognisers in the following python packages: NLTK, spacy\n",
        "\n",
        "a) people\n",
        "\n",
        "b) places\n",
        "\n",
        "c) organisations\n",
        "\n",
        "**CONSTRAINTS**\n",
        "The code must run in Google Colab.\n",
        "\n",
        "Do not change the name of the functions or their inputs.\n",
        "\n",
        "Your functions will be expected to return outputs as specified in the template functions.\n",
        "\n",
        "You may add additional functions as desired.\n",
        "\n",
        "Do not change the test cases at the bottom."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kV2yn3KLU68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff91146a-9749-4460-9819-29565cabd794"
      },
      "source": [
        "#Do not chance these dependencies\n",
        "import pytest\n",
        "# Import your dependencies here\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import re\n",
        "import datetime\n",
        "import json\n",
        "\n",
        "import nltk\n",
        "!pip install svgling\n",
        "import svgling\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: svgling in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "Requirement already satisfied: svgwrite in /usr/local/lib/python3.10/dist-packages (from svgling) (1.4.3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auv9_6KdK8HS"
      },
      "source": [
        "def bbc_scraper(url):\n",
        "    \"\"\"\n",
        "    This function should take a url, which will relate to a bbc news article\n",
        "    and return a json object containing the following fields:\n",
        "    1) URL (provided.  For example https://www.bbc.co.uk/news/uk-51004218)\n",
        "    2) Title\n",
        "    3) Date_published\n",
        "    4) Content --(the main body of article, this must be one continuous string without linebreaks)\n",
        "    The function must be iterable (If placed in a for loop and provided with several URLs in\n",
        "    turn return the correct json object for each time it is invoked without any manual intervention)\n",
        "    \"\"\"\n",
        "\n",
        "    page = requests.get(url)\n",
        "\n",
        "    soup = BeautifulSoup(page.text, 'lxml')\n",
        "\n",
        "    # # Title\n",
        "    title = soup.h1.text\n",
        "\n",
        "    # # Date_published\n",
        "    date = re.split('\"datePublished\":\"|\",\"dateModified\"', str(soup))[1][0:10]\n",
        "    date = datetime.datetime.strptime(date, '%Y-%m-%d').strftime('%d %B %Y')\n",
        "\n",
        "    # Content\n",
        "    plist = (soup.find_all('p'))\n",
        "    text = ''\n",
        "    i = 0\n",
        "    while i < (len(plist) - 5):\n",
        "      if 'e1jhz7w10' in str(plist[i]) and 'This video can not be played' not in plist[i]:\n",
        "        text = text + str(plist[i].text) + ' '\n",
        "        i = i + 1\n",
        "      else:\n",
        "        i = i + 2\n",
        "\n",
        "    # Remove final space\n",
        "    text = text[0:-1]\n",
        "\n",
        "    # # Build JSON\n",
        "    results_json = json.dumps(dict(URL = url, Title = title, Date_published = date, Content = text))\n",
        "\n",
        "    return results_json\n",
        "\n",
        "# TEST single URL\n",
        "# bbc_scraper('https://www.bbc.co.uk/news/uk-52255465')\n",
        "\n",
        "# TEST multiple URLs in for loop\n",
        "# artlist = ['https://www.bbc.co.uk/news/uk-52255054','https://www.bbc.co.uk/news/uk-51004218']\n",
        "# for i in artlist:\n",
        "#   print(bbc_scraper(i))"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDvLnOzZK79s"
      },
      "source": [
        "def extract_entities(string):\n",
        "    \"\"\"\n",
        "    This function should return a json containing the:\n",
        "    1) People\n",
        "    2) Places\n",
        "    3) Organisations\n",
        "    in the text string provided.\n",
        "    \"\"\"\n",
        "\n",
        "    tags = nltk.ne_chunk(pos_tag(word_tokenize(string)))\n",
        "\n",
        "    people = []\n",
        "    places = []\n",
        "    organisations = []\n",
        "\n",
        "    for i in tags:\n",
        "      if \"PERSON\" in str(i):\n",
        "        people.append(re.split('PERSON |/', str(i))[1])\n",
        "      elif \"GPE\" in str(i):\n",
        "        places.append(re.split('GPE |/', str(i))[1])\n",
        "      elif \"LOC\" in str(i):\n",
        "        places.append(re.split('LOC |/', str(i))[1])\n",
        "      elif \"ORGANIZATION\" in str(i):\n",
        "        organisations.append(re.split('ORGANIZATION |/', str(i))[1])\n",
        "\n",
        "    entities_json = json.dumps(dict(people = people, places = places, organisations = organisations))\n",
        "\n",
        "    return entities_json\n",
        "\n",
        "# TEST single string\n",
        "# extract_entities('I work for Amazon.')\n",
        "\n",
        "# TEST multiple strings in for loop\n",
        "# strlist = ['I work for Amazon.','My name is Bob']\n",
        "# for i in artlist:\n",
        "#   print(extract_entities(i))"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuiH-IhIK73H"
      },
      "source": [
        "####################################################################\n",
        "# Test cases\n",
        "\n",
        "def test_bbc_scrape():\n",
        "    results = {'URL': 'https://www.bbc.co.uk/news/uk-52255054',\n",
        "                'Title': 'Coronavirus: \\'We need Easter as much as ever,\\' says the Queen',\n",
        "                'Date_published': '11 April 2020',\n",
        "                'Content': '\"Coronavirus will not overcome us,\" the Queen has said, in an Easter message to the nation. While celebrations would be different for many this year, she said: \"We need Easter as much as ever.\" Referencing the tradition of lighting candles to mark the occasion, she said: \"As dark as death can be - particularly for those suffering with grief - light and life are greater.\" It comes as the number of coronavirus deaths in UK hospitals reached 9,875. Speaking from Windsor Castle, the Queen said many religions had festivals celebrating light overcoming darkness, which often featured the lighting of candles. She said: \"They seem to speak to every culture, and appeal to people of all faiths, and of none. \"They are lit on birthday cakes and to mark family anniversaries, when we gather happily around a source of light. It unites us.\" The monarch, who is head of the Church of England, said: \"As darkness falls on the Saturday before Easter Day, many Christians would normally light candles together.  \"In church, one light would pass to another, spreading slowly and then more rapidly as more candles are lit. It\\'s a way of showing how the good news of Christ\\'s resurrection has been passed on from the first Easter by every generation until now.\" As far as we know, this is the first time the Queen has released an Easter message. And coming as it does less than a week since the televised broadcast to the nation, it underlines the gravity of the situation as it is regarded by the monarch. It serves two purposes really; it is underlining the government\\'s public safety message, acknowledging Easter will be difficult for us but by keeping apart we keep others safe, and the broader Christian message of hope and reassurance.  We know how important her Christian faith is, and coming on the eve of Easter Sunday, it is clearly a significant time for people of all faiths, but particularly Christian faith. She said the discovery of the risen Christ on the first Easter Day gave his followers new hope and fresh purpose, adding that we could all take heart from this.  Wishing everyone of all faiths and denominations a blessed Easter, she said: \"May the living flame of the Easter hope be a steady guide as we face the future.\" The Queen, 93, recorded the audio message in the White Drawing Room at Windsor Castle, with one sound engineer in the next room.  The Palace described it as \"Her Majesty\\'s contribution to those who are celebrating Easter privately\".  It follows a speech on Sunday, in which the monarch delivered a rallying message to the nation. In it, she said the UK \"will succeed\" in its fight against the coronavirus pandemic, thanked people for following government rules about staying at home and praised those \"coming together to help others\". She also thanked key workers, saying \"every hour\" of work \"brings us closer to a return to more normal times\".'}\n",
        "    scraper_result = bbc_scraper('https://www.bbc.co.uk/news/uk-52255054')\n",
        "    assert json.loads(scraper_result) == results\n",
        "\n",
        "\n",
        "def test_extract_entities_amazon_org():\n",
        "    input_string = \"I work for Amazon.\"\n",
        "    results_dict = {'people':[],\n",
        "                    'places':[],\n",
        "                    'organisations': ['Amazon']\n",
        "                    }\n",
        "    extracted_entities_results = extract_entities(input_string)\n",
        "    assert json.loads(extracted_entities_results) == results_dict\n",
        "\n",
        "\n",
        "def test_extract_entities_name():\n",
        "    input_string = \"My name is Bob\"\n",
        "    results_dict = {'people':['Bob'],\n",
        "                    'places':[],\n",
        "                    'organisations': []\n",
        "                    }\n",
        "    extracted_entities_results = extract_entities(input_string)\n",
        "    assert json.loads(extracted_entities_results) == results_dict"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EtNG-mJK7pt"
      },
      "source": [
        "test_bbc_scrape()\n",
        "test_extract_entities_amazon_org()\n",
        "test_extract_entities_name()"
      ],
      "execution_count": 35,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}